---
title: "Assigment 1. Statistical Theory and Modeling"
format: html
authors: "Caroline Birkehammar, Pablo Paras Ochoa, Steven Hiram Rubio Vasquez"
editor: visual
---

## Assignment 1

Import of the libraries and data

```{r,warning=FALSE,message=FALSE}
#| echo: false
library(patchwork)
library(pacman)
p_load(readxl, tidyverse, ggplot2, janitor, e1071, scales, ggridges)

Sys.setlocale("LC_ALL", "es_ES.UTF-8") 
options(scipen=999) 

set.seed(42)

data = read.csv("https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv", header = TRUE) 


```

## Part 1

### Problem 1a

```{r, warning=FALSE,message=FALSE}

dist <- rexp(10000, 1/2)

mean(dist)

```

The true value for the mean given that $$ \lambda = 2 $$ or $$ \beta = 1/2 $$ is $$2$$ When drawing a sample of 10,000 we obtain $$\bar{x} = 2.013022$$ Therefore, we conclude that R is using the correct rate of parameterization.

### Problem 1b

```{r, warning=FALSE,message=FALSE}

dist <- tibble(rexp(200, 1/2)) %>% rename(X = 1)

dist_theo <- tibble(c(seq(from = 0, to = 20, by = .01)), dexp(c(seq(from = 0, to = 20, by = .01)), rate = 1/2)) %>% rename(X = 1, Y = 2)

ggplot() +
  geom_histogram(aes(dist$X, after_stat(density)), bins = 30, fill = "steelblue", color = "black") +
  geom_line(aes(dist_theo$X, dist_theo$Y)) +
  theme_minimal() +
  labs(title = "Histogram of exponential distribution with beta = 2",
       subtitle = "Comparing theoretical pdf with the distribution of randomly generated data") +
  xlab("X") +
  ylab("Probability") 


```

### Problem 1c

```{r, warning=FALSE,message=FALSE}

theo_data <- c(seq(from = 0, to = 20, by = .01))

dist_theo1 <- tibble(theo_data, dexp(theo_data, rate = 1/1)) %>% rename(X = 1, Y = 2) 

dist_theo2 <- tibble(theo_data, dexp(theo_data, rate = 1/2)) %>% rename(X = 1, Y = 2) 

dist_theo3 <- tibble(theo_data, dexp(theo_data, rate = 1/3)) %>% rename(X = 1, Y = 2) 


ggplot() +
  geom_histogram(aes(dist$X, after_stat(density)), bins = 30, fill = "steelblue", color = "black") +
  geom_line(aes(dist_theo1$X, dist_theo1$Y), color = "#FF9E0D") +
  geom_line(aes(dist_theo2$X, dist_theo2$Y), color = "#1BE2DC") +
  geom_line(aes(dist_theo3$X, dist_theo3$Y), color = "#09E920") +
  theme_minimal() +
  labs(title = "Histogram of exponential distribution with beta = 2",
       subtitle = "Comparing theoretical pdf with the distribution of randomly generated data") +
  xlab("X") +
  ylab("Probability") 


```

Out of the three curves, it is clear that the orange one using $$ \beta = 1$$ is the worse fit, as it underestimates the density of larger values and overestimates the density of smaller values. Out of the two other curves, using $$ \beta = 2 $$ for the blue one (which uses the same parameter value as when we generated a random sample) and $$ \beta = 3 $$ for the green one, it is not readily obvious which one is the better fit. The green pdf ($\beta = 3$) slightly overestimates large values and slightly underestimates small ones, but the blue pdf ($\beta = 2$) is also not a perfect fit.

### Problem 1d

```{r, warning=FALSE,message=FALSE}

dist_theo1 <- tibble(rexp(10000, 1/1)) %>% rename(X = 1)

dist_theo2 <- tibble(rexp(10000, 1/2)) %>% rename(X = 1)

dist_theo3 <- tibble(rexp(10000, 1/3)) %>% rename(X = 1)

ggplot() +
  stat_ecdf(aes(dist$X), color = "black") +
  stat_ecdf(aes(dist_theo1$X), color = "#FF9E0D") +
  stat_ecdf(aes(dist_theo2$X), color = "#1BE2DC") +
  stat_ecdf(aes(dist_theo3$X), color = "#09E920") +
  theme_minimal() +
  labs(title = "Cumulative distribution",
       subtitle = "Comparing theoretical cumulative distribution with randomly generated data") +
  xlab("X") +
  ylab("Cumulative probability") 

```

In this case it is more obvious which theoretical CDF is the better fit. The blue one ($\beta = 2$) is very closely aligned with the data while the orange ($\beta = 1$) and green ($\beta = 3$) are clearly different.

### Problem 1e

```{r, warning=FALSE,message=FALSE}
obs_median   <- median(dist$X)
theo_median1 <- log(2)/1        # Theoretical median where lambda = 1
theo_median2 <- log(2)/(1/2)    # Theoretical median where lambda = 1/2
theo_median3 <- log(2)/(1/3)    # Theoretical median where lambda = 1/3

print(obs_median)

print(theo_median1)

print(theo_median2)

print(theo_median3)

```

To obtain a sample median, one must simply order all observations by magnitude and select the value in the middle. Should there be an even number of observations, the two values in the middle can be averaged to obtain it.

For theoretical distributions we must do a little bit of math. In this case for the exponential distribution the median is defined as $$ median(x) = \frac{ln(2)}{\lambda} $$ and $$ \lambda = \frac{1}{\beta} $$ Above, we calculated the theoretical medians for $\lambda = 1$, $\lambda = 1/2$ and $\lambda = 1/3$ by simply plugging in those values into the formula to obtain our results.

### Problem 1f

```{r, warning=FALSE,message=FALSE}

delta <- c(5, 1, .5, .25, .1, .01, .001, .0001) # 5 and 1 also, just for fun :)

for (i in delta) {
  theo_data <- c(seq(from = 0, to = 20, by = i))
  dist_theo <- tibble(theo_data, dexp(theo_data, rate = 1/2)) %>% rename(X = 1, Y = 2)
  integral = sum(dist_theo$Y*i)
  print(paste("Density obtained using delta = ", i, ": ", round(integral, digits = 5), sep = ""))
}

```

### Problem 1g

```{r, warning=FALSE,message=FALSE}

expon_fun <- function(x) {
  y = (1/2)*exp(-x/2)
  return(y)
}

integrate(f = expon_fun, lower = 0, upper = Inf)
```

## Part 2

### Problem 2a

After reading the data set, we select the bugs column and calculate the expected value (Lambda), with this, we attempt to fit a Poisson regression to the data with different bin sizes. As for the number of bins, the rule ***sqrt**(number of observations)* was used as a starting point.

```{r}
y = data$nBugs 
lambda=mean(y)
lambda

```

```{r, warning=FALSE,message=FALSE}
#| echo: false
plot1 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 9,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(title = "Data and poisson distribution",
       subtitle = "Comparing the distribution of number of bugs and the poisson distribution with n=9") +
  xlab("Count of y") +
  ylab("Density")+
  xlim(0,35)

plot2 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 18,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(
       subtitle = "Comparing the distribution of number of bugs and the poisson distribution with n=18") +
  xlab("Count of y") +
  ylab("Density")+
  xlim(0,35)

plot3 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 27,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(
       subtitle = "Comparing the distribution of number of bugs and the poisson distribution with n=27") +
  xlab("Count of y") +
  ylab("Density)")+
  xlim(0,35)

plot1 / plot2 / plot3
```

It can be observed that the distribution does not follow the poisson distribution in any case. In the first two cases it resembles more a exponential distribution and in the last one it appears as it has two modes.

### Problem 2b

For the second task, we generate distribution curves for a Negative Binomial distribution and compare it with the previous result.

```{r, warning=FALSE,message=FALSE}
#| echo: false
df <- data.frame(VNeg1=rnbinom(10000,mu = lambda,size=1),
                 VNeg2=rnbinom(10000,mu = lambda,size=3),
                 VNeg3=rnbinom(10000,mu = lambda,size=100),
                 Pois=rpois(10000,lambda = lambda))
colors <- c("Poisson" = "#fc1b00","BNr1" = "#FF9E0D", "BNr3" = "#1BE2DC", "BNr100"="#09E920")

ggplot() + 
  geom_density(aes(df$Pois, color = "Poisson"), linewidth = 1.0, bw = 1.0)+ 
  geom_density(aes(df$VNeg1, color = "BNr1"), linewidth = 1.0, bw = 1.0) +
  geom_density(aes(df$VNeg2, color = "BNr3"), linewidth = 1.0,bw = 1.0) +
  geom_density(aes(df$VNeg3, color = "BNr100"), linewidth = 1.0,bw = 1.0) +
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 18,color="black",alpha=0.4, fill="#dde542")+
  theme_minimal() +
  labs(title = "Data, poisson and negative binomial distributions",
       subtitle = "Comparing the distribution of number of bugs and 4 theoretical curves",
       color = "Legend") +
  xlab("Count of y")+xlim(-1,40)+ylim(0,0.18)+
  ylab("Density")+scale_color_manual(values = colors,labels = c("BNr1"="NB r=1","BNr100"="NB r=100", "BNr3"="NB r=3","Poisson"="Poisson" ))

```

We can observe that, now, the curve that follows the distribution of the data better is the blue line (r=3), and the curve that's more similar to the poisson model is the green line (r=100). This is because...

## Part 3

### Problem 3a

***Let*** $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. We are now interested in the distribution of $Y=\exp(X)$. Obtain the distribution for $Y$ by simulating 10000 draws. Plot a histogram with 100 bins.

```{r}
#| echo: false

set.seed(123)
X <- rnorm(10000, mean = 0, sd = 1)
Y <- exp(X)

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65),
     main = "Distribution of Y in simulated data", col = "lightseagreen")

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65), xlim = c(0, 10),
     main = "Distribution of Y in simulated data, zoomed in", col = "lightseagreen")
```

### Problem 3b

***Use the method of transformation to show that the probability density for*** $Y$ is given by

$$
f(x)=\frac{1}{\sqrt{2\pi}x}\exp\Big(-\frac{1}{2}(\log(x)-\mu)^2\Big)
$$

***Overlay a plot of this density in the histogram from Problem 3a).***

We know that for any function $X = f(x)$ and $Y = g(X)$, where $g(X)$ is an invertible, differentiable and monotonically increasing or decreasing variable, the probability density function of $Y$ is given by $$
f_Y(y) = f_X\left(g^{-1}(y)\right) \left| \frac{d}{dy} g^{-1}(y) \right|,
$$ where $g^{-1}(y)$ is the inverse function of $Y$.

In our case where $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$, the probability density function of X is given by $$
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{1}{2\sigma^2}(x - \mu)^2 \right) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{x^2}{2} \right).
$$ $Y$ is a function of $X$, $Y=\exp(X)$, and has the inverse function $X = log(Y)$. The derivative of the inverse function is given by

$$
\frac{d}{dy} g^{-1}(y) = \frac{d}{dy} \log(y) = \frac{1}{y}.
$$

Replacing into the equation for the PDF of Y gives:

$$
f_Y(y) = f_X\left(g^{-1}(y)\right) \left| \frac{d}{dy} g^{-1}(y) \right| = f_X(\log(y)) \cdot \frac{1}{y} = \frac{1}{y\sqrt{2\pi}} \exp\left( -\frac{\log(y)^2}{2} \right)
$$

Below are two versions of the histograms from problem 3a with overlaid theoretical probability distributions; the first one is in full scale and with 100 bins, and the second one is zoomed in and has more bins to better show the distribution of the simulated data.

```{r}
#| echo: false

y_pdf <- function(y) {1 / (y * sqrt(2 * pi)) * exp(-0.5 * (log(y))^2)}

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65),
     main = "Simulated data and theoretical PDF of Y", col = "lightseagreen")
curve(y_pdf(x), from = min(Y), to = max(Y), col = "red", lwd = 2, add = TRUE)

hist(Y, breaks = 500, freq = F, ylim = c(0, 0.65), xlim = c(0, 10),
     main = "Simulated data and theoretical PDF of Y, zoomed in", col = "lightseagreen")
curve(y_pdf(x), from = min(Y), to = 10, col = "red", lwd = 2, add = TRUE)
```

### Problem 3c

***Use Monte Carlo simulation with*** $m=10000$ random draws to estimate $\mathrm{E}(Y)$ where $Y=\exp(X)$ and $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. Check the convergence of the estimate by plotting the sequential Monte Carlo estimates for increasing Monte Carlo sample sizes of $10,20,30,\ldots,9900, 10000$. Does the estimate seem to converge (settle down) to the true expectation, which happens to be $\mathrm{E}(Y)=\exp(\frac{1}{2})$?

```{r}
#| echo: false

set.seed(123)

Y_mean   <- exp(0.5)
mc_means <- sapply(seq(10, 10000, by = 10), function(n) mean(Y[1:n]))

plot(seq(10, 10000, by = 10), mc_means, type = "l", col = "lightseagreen",
     main = "Monte Carlo Convergence of E(Y)",
     xlab = "Sample Size", ylab = "Estimated Expected Value",
     ylim = range(c(mc_means, Y_mean)))
abline(h = Y_mean, col = "black", lwd = 2, lty = 2)
legend("topright", col = c("lightseagreen", "black"), lwd = 2, lty = c(1, 2),
       legend = c("Monte Carlo Expected Value Estimate", "True Expected Value exp(0.5) ~ 1.6487"))
```

Monte Carlo Simulation relies on repeated random sampling to obtain some estimated value, in this case the expected value of $Y=\exp(X)$ where $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. The plot above shows how the average mean value of $Y$ converges to the true mean (which is approximately 1.6587) as sample size increases. There are 1000 data points in the plot since each data point includes 10 more observations and we have generated random data with 10000 observations in total. We note that after about the 200th data point, which averages 2000 observations, the Monte Carlo estimate of the expected value do not fluctuate as much as it did for smaller sample sizes, and pretty much converges to the true mean.
