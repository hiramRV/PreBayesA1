---
title: "Assigment 1. Statistical Theory and Modeling"
format: html
authors: "Caroline Birkehammar, Pablo Paras Ochoa, Steven Hiram Rubio Vasquez"
editor: visual
---

## Assignment 1

Import of the libraries and data

```{r,warning=FALSE,message=FALSE}
#| echo: false
library(patchwork)
library(pacman)
p_load(readxl, tidyverse, ggplot2, janitor, e1071, scales, ggridges)

Sys.setlocale("LC_ALL", "es_ES.UTF-8") 
options(scipen=999) 

set.seed(42)

data = read.csv("https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv", header = TRUE) 


```

## Part 1

### Problem 1a

```{r, warning=FALSE,message=FALSE}

dist <- rexp(10000, 1/2)

mean(dist)

```

The true value for the mean given that $\lambda = 2$ or $\beta = 1/2$ is $2$. When drawing a sample of 10,000 we obtain $\bar{x} = 2.013022$. Therefore, we conclude that R is using the correct rate of parameterization.

### Problem 1b

```{r, warning=FALSE,message=FALSE}

dist <- tibble(rexp(200, 1/2)) %>% rename(X = 1)

dist_theo <- tibble(c(seq(from = 0, to = 20, by = .01)), dexp(c(seq(from = 0, to = 20, by = .01)), rate = 1/2)) %>% rename(X = 1, Y = 2)

ggplot() +
  geom_histogram(aes(dist$X, after_stat(density)), bins = 30, fill = "steelblue", color = "black") +
  geom_line(aes(dist_theo$X, dist_theo$Y)) +
  theme_minimal() +
  labs(title = "Histogram of exponential distribution with beta = 2",
       subtitle = "Comparing theoretical pdf with the distribution of randomly generated data") +
  xlab("X") +
  ylab("Probability") 


```

### Problem 1c

```{r, warning=FALSE,message=FALSE}

theo_data <- c(seq(from = 0, to = 20, by = .01))

dist_theo1 <- tibble(theo_data, dexp(theo_data, rate = 1/1)) %>% rename(X = 1, Y = 2) 

dist_theo2 <- tibble(theo_data, dexp(theo_data, rate = 1/2)) %>% rename(X = 1, Y = 2) 

dist_theo3 <- tibble(theo_data, dexp(theo_data, rate = 1/3)) %>% rename(X = 1, Y = 2) 


ggplot() +
  geom_histogram(aes(dist$X, after_stat(density)), bins = 30, fill = "steelblue", color = "black") +
  geom_line(aes(dist_theo1$X, dist_theo1$Y), color = "#FF9E0D") +
  geom_line(aes(dist_theo2$X, dist_theo2$Y), color = "#1BE2DC") +
  geom_line(aes(dist_theo3$X, dist_theo3$Y), color = "#09E920") +
  theme_minimal() +
  labs(title = "Histogram of exponential distribution with beta = 2",
       subtitle = "Comparing theoretical pdf with the distribution of randomly generated data") +
  xlab("X") +
  ylab("Probability") 


```

Out of the three curves, it is clear that the orange one using $\beta = 1$ is the worse fit, as it underestimates the density of larger values and overestimates the density of smaller values. Out of the two other curves, using $\beta = 2$ for the blue one (which uses the same parameter value as when we generated a random sample) and $\beta = 3$ for the green one, it is not readily obvious which one is the better fit. The green pdf ($\beta = 3$) slightly overestimates large values and slightly underestimates small ones, but the blue pdf ($\beta = 2$) is also not a perfect fit.

### Problem 1d

```{r, warning=FALSE,message=FALSE}

dist_theo1 <- tibble(rexp(10000, 1/1)) %>% rename(X = 1)

dist_theo2 <- tibble(rexp(10000, 1/2)) %>% rename(X = 1)

dist_theo3 <- tibble(rexp(10000, 1/3)) %>% rename(X = 1)

ggplot() +
  stat_ecdf(aes(dist$X), color = "black") +
  stat_ecdf(aes(dist_theo1$X), color = "#FF9E0D") +
  stat_ecdf(aes(dist_theo2$X), color = "#1BE2DC") +
  stat_ecdf(aes(dist_theo3$X), color = "#09E920") +
  theme_minimal() +
  labs(title = "Cumulative distribution",
       subtitle = "Comparing theoretical cumulative distribution with randomly generated data") +
  xlab("X") +
  ylab("Cumulative probability") 

```

In this case it is more obvious which theoretical CDF is the better fit. The blue one ($\beta = 2$) is very closely aligned with the data while the orange ($\beta = 1$) and green ($\beta = 3$) are clearly different.

### Problem 1e

```{r, warning=FALSE,message=FALSE}
obs_median   <- median(dist$X)
theo_median1 <- log(2)/1        # Theoretical median where lambda = 1
theo_median2 <- log(2)/(1/2)    # Theoretical median where lambda = 1/2
theo_median3 <- log(2)/(1/3)    # Theoretical median where lambda = 1/3

print(obs_median)

print(theo_median1)

print(theo_median2)

print(theo_median3)

```

To obtain a sample median, one must simply order all observations by magnitude and select the value in the middle. Should there be an even number of observations, the two values in the middle can be averaged to obtain it.

For theoretical distributions we must do a little bit of math. In this case for the exponential distribution the median is defined as $$ median(x) = \frac{ln(2)}{\lambda} $$ and $$ \lambda = \frac{1}{\beta} $$ Above, we calculated the theoretical medians for $\lambda = 1$, $\lambda = 1/2$ and $\lambda = 1/3$ by simply plugging in those values into the formula to obtain our results.

### Problem 1f

```{r, warning=FALSE,message=FALSE}

delta <- c(5, 1, .5, .25, .1, .01, .001, .0001) # 5 and 1 also, just for fun :)

for (i in delta) {
  theo_data <- c(seq(from = 0, to = 20, by = i))
  dist_theo <- tibble(theo_data, dexp(theo_data, rate = 1/2)) %>% rename(X = 1, Y = 2)
  integral = sum(dist_theo$Y*i)
  print(paste("Density obtained using delta = ", i, ": ", round(integral, digits = 5), sep = ""))
}

```

### Problem 1g

```{r, warning=FALSE,message=FALSE}

expon_fun <- function(x) {
  y = (1/2)*exp(-x/2)
  return(y)
}

integrate(f = expon_fun, lower = 0, upper = Inf)
```

## Part 2

### Problem 2a

After reading the data set, we select the bugs column and calculate the expected value ($\lambda$) and fit a Poisson regression to the data. As for the number of bins, the rule ***sqrt**(number of observations)* was used as a starting point, but we tried different bin sizes.

```{r}
y <- data$nBugs 
lambda <- mean(y)
lambda

```

```{r, warning=FALSE,message=FALSE}
#| echo: false
plot1 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 9,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(title = "Data and Poisson distribution",
       subtitle = "Comparing the distribution of number of bugs and the Poisson distribution with n = 9") +
  xlab("y") +
  ylab("Density")+
  xlim(0,35)

plot2 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 18,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(
       subtitle = "Comparing the distribution of number of bugs and the Poisson distribution with n = 18") +
  xlab("y") +
  ylab("Density")+
  xlim(0,35)

plot3 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 27,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(
       subtitle = "Comparing the distribution of number of bugs and the Poisson distribution with n = 27") +
  xlab("y") +
  ylab("Density")+
  xlim(0,35)

plot1 / plot2 / plot3
```

The distribution does not seem to follow the Poisson distribution, based on how the histograms look compared to the theoretical probability distribution.

### Problem 2b

We fit the data to a Negative Binomial distribution and compare it with the previous result.

```{r, warning=FALSE,message=FALSE}
#| echo: false
df <- data.frame(VNeg1=rnbinom(10000,mu = lambda,size=1),
                 VNeg2=rnbinom(10000,mu = lambda,size=3),
                 VNeg3=rnbinom(10000,mu = lambda,size=100),
                 Pois=rpois(10000,lambda = lambda))
colors <- c("Poisson" = "#fc1b00","BNr1" = "#FF9E0D", "BNr3" = "#1BE2DC", "BNr100"="#09E920")

ggplot() + 
  geom_density(aes(df$Pois, color = "Poisson"), linewidth = 1.0, bw = 1.0)+ 
  geom_density(aes(df$VNeg1, color = "BNr1"), linewidth = 1.0, bw = 1.0) +
  geom_density(aes(df$VNeg2, color = "BNr3"), linewidth = 1.0,bw = 1.0) +
  geom_density(aes(df$VNeg3, color = "BNr100"), linewidth = 1.0,bw = 1.0) +
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 18,color="black",alpha=0.4, fill="#dde542")+
  theme_minimal() +
  labs(title = "Histogram of bug data, theoretical Poisson and Negative Binomial distributions",
       subtitle = "Comparing the distribution of number of bugs and 4 theoretical curves",
       color = "Distributions") +
  xlab("y")+xlim(-1,40)+ylim(0,0.18)+
  ylab("Density")+scale_color_manual(values = colors,labels = c("BNr1"="NegBin r = 1", "BNr100"="NegBin r = 100", "BNr3"="NegBin r = 3", "Poisson"="Poisson lambda = 5.252747" ))

```

We can observe that, now, the curve that follows the distribution of the data better is the blue line (r=3), and the curve that's more similar to the poisson model is the green line (r=100). This is because...

## Part 3

### Problem 3a

```{r}
#| echo: false

set.seed(123)
X <- rnorm(10000, mean = 0, sd = 1)
Y <- exp(X)

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65),
     main = "Distribution of Y in simulated data", col = "lightseagreen")

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65), xlim = c(0, 10),
     main = "Distribution of Y in simulated data, zoomed in", col = "lightseagreen")
```

### Problem 3b

We know that for any function $X = f(x)$ and $Y = g(X)$, where $g(X)$ is an invertible, differentiable and monotonically increasing or decreasing variable, the probability density function of $Y$ is given by $$
f_Y(y) = f_X\left(g^{-1}(y)\right) \left| \frac{d}{dy} g^{-1}(y) \right|,
$$ where $g^{-1}(y)$ is the inverse function of $Y$.

In our case where $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$, the probability density function of X is given by $$
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{1}{2\sigma^2}(x - \mu)^2 \right) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{x^2}{2} \right).
$$ $Y$ is a function of $X$, $Y=\exp(X)$, and has the inverse function $X = log(Y)$. The derivative of the inverse function is given by

$$
\frac{d}{dy} g^{-1}(y) = \frac{d}{dy} \log(y) = \frac{1}{y}.
$$

Replacing into the equation for the PDF of Y gives:

$$
f_Y(y) = f_X\left(g^{-1}(y)\right) \left| \frac{d}{dy} g^{-1}(y) \right| = f_X(\log(y)) \cdot \frac{1}{y} = \frac{1}{y\sqrt{2\pi}} \exp\left( -\frac{\log(y)^2}{2} \right)
$$

Below are two versions of the histograms from problem 3a with overlaid theoretical probability distributions; the first one is in full scale and with 100 bins, and the second one is zoomed in and has more bins to better show the distribution of the simulated data.

```{r}
#| echo: false

y_pdf <- function(y) {1 / (y * sqrt(2 * pi)) * exp(-0.5 * (log(y))^2)}

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65),
     main = "Simulated data and theoretical PDF of Y", col = "lightseagreen")
curve(y_pdf(x), from = min(Y), to = max(Y), col = "red", lwd = 2, add = TRUE)

hist(Y, breaks = 500, freq = F, ylim = c(0, 0.65), xlim = c(0, 10),
     main = "Simulated data and theoretical PDF of Y, zoomed in", col = "lightseagreen")
curve(y_pdf(x), from = min(Y), to = 10, col = "red", lwd = 2, add = TRUE)
```

### Problem 3c

```{r}
#| echo: false

set.seed(123)

Y_mean   <- exp(0.5)
mc_means <- sapply(seq(10, 10000, by = 10), function(n) mean(Y[1:n]))

plot(seq(10, 10000, by = 10), mc_means, type = "l", col = "lightseagreen",
     main = "Monte Carlo Convergence of E(Y)",
     xlab = "Sample Size", ylab = "Estimated Expected Value",
     ylim = range(c(mc_means, Y_mean)))
abline(h = Y_mean, col = "black", lwd = 2, lty = 2)
legend("topright", col = c("lightseagreen", "black"), lwd = 2, lty = c(1, 2),
       legend = c("Monte Carlo Expected Value Estimate", "True Expected Value exp(0.5) ~ 1.6487"))
```

Monte Carlo Simulation relies on repeated random sampling to obtain some estimated value, in this case the expected value of $Y=\exp(X)$ where $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. The plot above shows how the average mean value of $Y$ converges to the true mean (which is approximately 1.6587) as sample size increases. There are only 1,000 data points in the plot since each data point includes 10 observations and we have generated random data containing 10,000. After about the 200th data point (which averages 2,000 observations) the Monte Carlo estimate of the expected value stabilizes and converges around the true mean.
