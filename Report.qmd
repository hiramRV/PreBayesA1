---
title: "Assigment 1. Statistical Theory and Modeling"
format: html
authors: "Caroline Birkehammar, Pablo Paras Ochoa, Steven Hiram Rubio Vasquez"
editor: visual
---

## Assignment 1

Import of the libraries and data...

```{r,warning=FALSE,message=FALSE}
#| echo: false
library(patchwork)
library(pacman)
p_load(readxl, tidyverse, ggplot2, janitor, e1071, scales, ggridges)

Sys.setlocale("LC_ALL", "es_ES.UTF-8") 
options(scipen=999) 

set.seed(42)

data = read.csv("https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv", header = TRUE) 


```

## Part 1

1A:

```{r, warning=FALSE,message=FALSE}

dist <- rexp(10000, 1/2)

mean(dist)


```

1B:

```{r, warning=FALSE,message=FALSE}

dist <- tibble(rexp(200, 1/2)) %>% rename(X = 1)

dist_theo <- tibble(c(seq(from = 0, to = 20, by = .01)), dexp(c(seq(from = 0, to = 20, by = .01)), rate = 1/2)) %>% rename(X = 1, Y = 2)

ggplot() +
  geom_histogram(aes(dist$X, after_stat(density)), bins = 30, fill = "steelblue", color = "black") +
  geom_line(aes(dist_theo$X, dist_theo$Y)) +
  theme_minimal() +
  labs(title = "Histogram of the distribution",
       subtitle = "Comparing theoretical distribution with randomly generated data") +
  xlab("Count of X") +
  ylab("X") 


```

1C:

```{r, warning=FALSE,message=FALSE}

theo_data <- c(seq(from = 0, to = 20, by = .01))

dist_theo1 <- tibble(theo_data, dexp(theo_data, rate = 1/1)) %>% rename(X = 1, Y = 2) 

dist_theo2 <- tibble(theo_data, dexp(theo_data, rate = 1/2)) %>% rename(X = 1, Y = 2) 

dist_theo3 <- tibble(theo_data, dexp(theo_data, rate = 1/3)) %>% rename(X = 1, Y = 2) 


ggplot() +
  geom_histogram(aes(dist$X, after_stat(density)), bins = 30, fill = "steelblue", color = "black") +
  geom_line(aes(dist_theo1$X, dist_theo1$Y), color = "#FF9E0D", linetype = "dotdash") +
  geom_line(aes(dist_theo2$X, dist_theo2$Y), color = "#1BE2DC") +
  geom_line(aes(dist_theo3$X, dist_theo3$Y), color = "#09E920", linetype = "dotdash") +
  theme_minimal() +
  labs(title = "Histogram of the distribution",
       subtitle = "Comparing theoretical distribution with randomly generated data") +
  xlab("Count of X") +
  ylab("X") 


```

1D:

```{r, warning=FALSE,message=FALSE}

dist_theo1 <- tibble(rexp(10000, 1/1)) %>% rename(X = 1)

dist_theo2 <- tibble(rexp(10000, 1/2)) %>% rename(X = 1)

dist_theo3 <- tibble(rexp(10000, 1/3)) %>% rename(X = 1)

ggplot() +
  stat_ecdf(aes(dist$X), color = "black") +
  stat_ecdf(aes(dist_theo1$X), color = "#FF9E0D") +
  stat_ecdf(aes(dist_theo2$X), color = "#1BE2DC") +
  stat_ecdf(aes(dist_theo3$X), color = "#09E920") +
  theme_minimal() +
  labs(title = "Cumulative distribution",
       subtitle = "Comparing theoretical cumulative distribution with randomly generated data") +
  xlab("Count of X") +
  ylab("X") 


```

## Part 2

a\) After reading the data set, we select the bugs column and calculate the expected value (Lambda), with this, we attempt to fit a Poisson regression to the data with different bin sizes. As for the number of bins, the rule ***sqrt**(number of observations)* was used as a starting point.

```{r}
y = data$nBugs 
lambda=mean(y)
lambda

```

```{r, warning=FALSE,message=FALSE}
#| echo: false
plot1 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 9,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(title = "Data and poisson distribution",
       subtitle = "Comparing the distribution of number of bugs and the poisson distribution with n=9") +
  xlab("Count of y") +
  ylab("Pr(y<Y)")+
  xlim(0,35)

plot2 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 18,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(
       subtitle = "Comparing the distribution of number of bugs and the poisson distribution with n=18") +
  xlab("Count of y") +
  ylab("Pr(y<Y)")+
  xlim(0,35)

plot3 <- ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 27,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(
       subtitle = "Comparing the distribution of number of bugs and the poisson distribution with n=27") +
  xlab("Count of y") +
  ylab("Pr(y<Y)")+
  xlim(0,35)

plot1 / plot2 / plot3
```

It can be observed that the distribution does not follow the poisson distribution in any case. In the first two cases it resembles more a exponential distribution and in the last one it appears as it has two modes.

b\) For the second task, we generate distribution curves for a Negative Binomial distribution and compare it with the previous result.

```{r, warning=FALSE,message=FALSE}
#| echo: false
df <- data.frame(VNeg1=rnbinom(10000,mu = lambda,size=1),
                 VNeg2=rnbinom(10000,mu = lambda,size=3),
                 VNeg3=rnbinom(10000,mu = lambda,size=100),
                 Pois=rpois(10000,lambda = lambda))
colors <- c("Poisson" = "#fc1b00","BNr1" = "#FF9E0D", "BNr3" = "#1BE2DC", "BNr100"="#09E920")

ggplot() + 
  geom_density(aes(df$Pois, color = "Poisson"), linewidth = 1.0, bw = 1.0)+ 
  geom_density(aes(df$VNeg1, color = "BNr1"), linewidth = 1.0, bw = 1.0) +
  geom_density(aes(df$VNeg2, color = "BNr3"), linewidth = 1.0,bw = 1.0) +
  geom_density(aes(df$VNeg3, color = "BNr100"), linewidth = 1.0,bw = 1.0) +
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 18,color="black",alpha=0.4, fill="#dde542")+
  theme_minimal() +
  labs(title = "Data, poisson and negative binomial distributions",
       subtitle = "Comparing the distribution of number of bugs and 4 theoretical curves",
       color = "Legend") +
  xlab("Count of y")+xlim(-1,40)+ylim(0,0.18)+
  ylab("Pr(y<Y)")+scale_color_manual(values = colors,labels = c("BNr1"="NB r=1","BNr100"="NB r=100", "BNr3"="NB r=3","Poisson"="Poisson" ))

```

We can observe that, now, the curve that follows the distribution of the data better is the blue line (r=3), and the curve that's more similar to the poisson model is the green line (r=100). This is because...

## Part 3

### Problem 3a

***Let*** $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. We are now interested in the distribution of $Y=\exp(X)$. Obtain the distribution for $Y$ by simulating 10000 draws. Plot a histogram with 100 bins.

```{r}
#| echo: false

set.seed(123)
X <- rnorm(10000, mean = 0, sd = 1)
Y <- exp(X)

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65),
     main = "Distribution of Y in simulated data", col = "lightseagreen")

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65), xlim = c(0, 10),
     main = "Distribution of Y in simulated data, zoomed in", col = "lightseagreen")
```

### Problem 3b

***Use the method of transformation to show that the probability density for*** $Y$ is given by

$$
f(x)=\frac{1}{\sqrt{2\pi}x}\exp\Big(-\frac{1}{2}(\log(x)-\mu)^2\Big)
$$

***Overlay a plot of this density in the histogram from Problem 3a).***

We know that for any function $X = f(x)$ and $Y = g(X)$, where $g(X)$ is an invertible, differentiable and monotonically increasing or decreasing variable, the probability density function of $Y$ is given by $$
f_Y(y) = f_X\left(g^{-1}(y)\right) \left| \frac{d}{dy} g^{-1}(y) \right|,
$$ where $g^{-1}(y)$ is the inverse function of $Y$.

In our case where $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$, the probability density function of X is given by $$
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{1}{2\sigma^2}(x - \mu)^2 \right) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{x^2}{2} \right).
$$ $Y$ is a function of $X$, $Y=\exp(X)$, and has the inverse function $X = log(Y)$. The derivative of the inverse function is given by

$$
\frac{d}{dy} g^{-1}(y) = \frac{d}{dy} \log(y) = \frac{1}{y}.
$$

Replacing into the equation for the PDF of Y gives:

$$
f_Y(y) = f_X\left(g^{-1}(y)\right) \left| \frac{d}{dy} g^{-1}(y) \right| = f_X(\log(y)) \cdot \frac{1}{y} = \frac{1}{y\sqrt{2\pi}} \exp\left( -\frac{\log(y)^2}{2} \right)
$$

Below are two versions of the histograms from problem 3a with overlaid theoretical probability distributions; the first one is in full scale and with 100 bins, and the second one is zoomed in and has more bins to better show the distribution of the simulated data.

```{r}
#| echo: false

y_pdf <- function(y) {1 / (y * sqrt(2 * pi)) * exp(-0.5 * (log(y))^2)}

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65),
     main = "Simulated data and theoretical PDF of Y", col = "lightseagreen")
curve(y_pdf(x), from = min(Y), to = max(Y), col = "red", lwd = 2, add = TRUE)

hist(Y, breaks = 500, freq = F, ylim = c(0, 0.65), xlim = c(0, 10),
     main = "Simulated data and theoretical PDF of Y, zoomed in", col = "lightseagreen")
curve(y_pdf(x), from = min(Y), to = 10, col = "red", lwd = 2, add = TRUE)
```

### Problem 3c

***Use Monte Carlo simulation with*** $m=10000$ random draws to estimate $\mathrm{E}(Y)$ where $Y=\exp(X)$ and $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. Check the convergence of the estimate by plotting the sequential Monte Carlo estimates for increasing Monte Carlo sample sizes of $10,20,30,\ldots,9900, 10000$. Does the estimate seem to converge (settle down) to the true expectation, which happens to be $\mathrm{E}(Y)=\exp(\frac{1}{2})$?

```{r}
#| echo: false

set.seed(123)

Y_mean   <- exp(0.5)
mc_means <- sapply(seq(10, 10000, by = 10), function(n) mean(Y[1:n]))

plot(seq(10, 10000, by = 10), mc_means, type = "l", col = "lightseagreen",
     main = "Monte Carlo Convergence of E(Y)",
     xlab = "Sample Size", ylab = "Estimated Expected Value",
     ylim = range(c(mc_means, Y_mean)))
abline(h = Y_mean, col = "black", lwd = 2, lty = 2)
legend("topright", col = c("lightseagreen", "black"), lwd = 2, lty = c(1, 2),
       legend = c("Monte Carlo Expected Value Estimate", "True Expected Value exp(0.5) ~ 1.6487"))
```

Monte Carlo Simulation relies on repeated random sampling to obtain some estimated value, in this case the expected value of $Y=\exp(X)$ where $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. The plot above shows how the average mean value of $Y$ converges to the true mean (which is approximately 1.6587) as sample size increases. There are 1000 data points in the plot since each data point includes 10 more observations and we have generated random data with 10000 observations in total. We note that after about the 200th data point, which averages 2000 observations, the Monte Carlo estimate of the expected value do not fluctuate as much as it did for smaller sample sizes, and pretty much converges to the true mean.
