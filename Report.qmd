---
title: "Assigment 1. Statistical Theory and Modeling"
format: html
authors: "Caroline Birkehammar, Pablo Paras Ochoa, Steven Hiram Rubio Vasquez"
editor: visual
---

## Assignment 1

Import of the libraries and data...

```{r,warning=FALSE,message=FALSE}
#| echo: false

library(pacman)
p_load(readxl, tidyverse, ggplot2, janitor, e1071, scales, ggridges)

Sys.setlocale("LC_ALL", "es_ES.UTF-8") 
options(scipen=999) 

set.seed(42)

data = read.csv("https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv", header = TRUE) 


```

## Part 1

1A:

```{r, warning=FALSE,message=FALSE}

dist <- rexp(10000, 1/2)

mean(dist)


```

1B:

```{r, warning=FALSE,message=FALSE}

dist <- tibble(rexp(200, 1/2)) %>% rename(X = 1)

dist_theo <- tibble(c(seq(from = 0, to = 20, by = .01)), dexp(c(seq(from = 0, to = 20, by = .01)), rate = 1/2)) %>% rename(X = 1, Y = 2)

ggplot() +
  geom_histogram(aes(dist$X, after_stat(density)), bins = 30, fill = "steelblue", color = "black") +
  geom_line(aes(dist_theo$X, dist_theo$Y)) +
  theme_minimal() +
  labs(title = "Histogram of the distribution",
       subtitle = "Comparing theoretical distribution with randomly generated data") +
  xlab("Count of X") +
  ylab("X") 


```

1C:

```{r, warning=FALSE,message=FALSE}

theo_data <- c(seq(from = 0, to = 20, by = .01))

dist_theo1 <- tibble(theo_data, dexp(theo_data, rate = 1/1)) %>% rename(X = 1, Y = 2) 

dist_theo2 <- tibble(theo_data, dexp(theo_data, rate = 1/2)) %>% rename(X = 1, Y = 2) 

dist_theo3 <- tibble(theo_data, dexp(theo_data, rate = 1/3)) %>% rename(X = 1, Y = 2) 


ggplot() +
  geom_histogram(aes(dist$X, after_stat(density)), bins = 30, fill = "steelblue", color = "black") +
  geom_line(aes(dist_theo1$X, dist_theo1$Y), color = "#FF9E0D", linetype = "dotdash") +
  geom_line(aes(dist_theo2$X, dist_theo2$Y), color = "#1BE2DC") +
  geom_line(aes(dist_theo3$X, dist_theo3$Y), color = "#09E920", linetype = "dotdash") +
  theme_minimal() +
  labs(title = "Histogram of the distribution",
       subtitle = "Comparing theoretical distribution with randomly generated data") +
  xlab("Count of X") +
  ylab("X") 


```

1D:

```{r, warning=FALSE,message=FALSE}

dist_theo1 <- tibble(rexp(10000, 1/1)) %>% rename(X = 1)

dist_theo2 <- tibble(rexp(10000, 1/2)) %>% rename(X = 1)

dist_theo3 <- tibble(rexp(10000, 1/3)) %>% rename(X = 1)

ggplot() +
  stat_ecdf(aes(dist$X), color = "black") +
  stat_ecdf(aes(dist_theo1$X), color = "#FF9E0D") +
  stat_ecdf(aes(dist_theo2$X), color = "#1BE2DC") +
  stat_ecdf(aes(dist_theo3$X), color = "#09E920") +
  theme_minimal() +
  labs(title = "Cumulative distribution",
       subtitle = "Comparing theoretical cumulative distribution with randomly generated data") +
  xlab("Count of X") +
  ylab("X") 


```

## Part 2

a\) After reading the bugs data and calculating lambda we try to fit an EXP distribution:

```{r}
y = data$nBugs # number of bugs, a vector with n = 91 observations
lambda=mean(y)

ggplot() + 
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 20,color="black",alpha=0.4, fill="#dde542")+
  geom_function(fun = function(x) lambda^x*exp(-lambda)/factorial(x))+
  theme_minimal() +
  labs(title = "Data and poisson distribution",
       subtitle = "Comparing the distribution of number of bugs and the poisson distribution") +
  xlab("Count of y") +
  ylab("Pr(y<Y)")
```

The data does NOT follow a Poisson distribution.

b\) We generate the data and added on top of the previous one

```{r}
df <- data.frame(VNeg1=rnbinom(500,mu = lambda,size=1),
                 VNeg2=rnbinom(500,mu = lambda,size=3),
                 VNeg3=rnbinom(500,mu = lambda,size=100),
                 Pois=rpois(500,lambda = lambda))
colors <- c("Poisson" = "#fc1b00","BNr1" = "#FF9E0D", "BNr3" = "#1BE2DC", "BNr100"="#09E920")

ggplot() + 
  geom_density(aes(df$Pois, color = "Poisson"), linewidth = 1.0 )+ 
  geom_density(aes(df$VNeg1, color = "BNr1"), linewidth = 1.0 ) +
  geom_density(aes(df$VNeg2, color = "BNr3"), linewidth = 1.0) +
  geom_density(aes(df$VNeg3, color = "BNr100"), linewidth = 1.0) +
  geom_histogram(aes(data$nBugs,after_stat(density)),bins = 20,color="black",alpha=0.4, fill="#dde542")+
  theme_minimal() +
  labs(title = "Data, poisson and negative binomial distributions",
       subtitle = "Comparing the distribution of number of bugs and 4 theoretical curves",
       color = "Legend") +
  xlab("Count of y") +
  ylab("Pr(y<Y)")+scale_color_manual(values = colors,labels = c("BNr1"="NB r=1","BNr100"="NB r=100", "BNr3"="NB r=3","Poisson"="Poisson" ))

```

The curve that follows the distribution of the data better is the blue line (r=3), and the curve that's more similar to the poisson model is the green line (r=100) because bigger r's approximates to it?

## Part 3

### Problem 3a

***Let $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. We are now interested in the distribution of $Y=\exp(X)$. Obtain the distribution for $Y$ by simulating 10000 draws. Plot a histogram with 100 bins.***

```{r}
#| echo: false

set.seed(123)
X <- rnorm(10000, mean = 0, sd = 1)
Y <- exp(X)

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65),
     main = "Distribution of Y in simulated data", col = "lightseagreen")

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65), xlim = c(0, 10),
     main = "Distribution of Y in simulated data, zoomed in", col = "lightseagreen")
```



### Problem 3b


***Use the method of transformation to show that the probability density for $Y$ is given by***

$$
f(x)=\frac{1}{\sqrt{2\pi}x}\exp\Big(-\frac{1}{2}(\log(x)-\mu)^2\Big)
$$

***Overlay a plot of this density in the histogram from Problem 3a).***

We know that for any function $X = f(x)$ and $Y = g(X)$, where $g(X)$ is an invertible, differentiable and monotonically increasing or decreasing variable, the probability density function of $Y$ is given by
$$
f_Y(y) = f_X\left(g^{-1}(y)\right) \left| \frac{d}{dy} g^{-1}(y) \right|,
$$
where $g^{-1}(y)$ is the inverse function of $Y$.

In our case where $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$, the probability density function of X is given by
$$
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{1}{2\sigma^2}(x - \mu)^2 \right) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{x^2}{2} \right).
$$
$Y$ is a function of $X$, $Y=\exp(X)$, and has the inverse function $X = log(Y)$. The derivative of the inverse function is given by

$$
\frac{d}{dy} g^{-1}(y) = \frac{d}{dy} \log(y) = \frac{1}{y}.
$$

Replacing into the equation for the PDF of Y gives: 

$$
f_Y(y) = f_X\left(g^{-1}(y)\right) \left| \frac{d}{dy} g^{-1}(y) \right| = f_X(\log(y)) \cdot \frac{1}{y} = \frac{1}{y\sqrt{2\pi}} \exp\left( -\frac{\log(y)^2}{2} \right)
$$

Below are two versions of the histograms from problem 3a with overlaid theoretical probability distributions; the first one is in full scale and with 100 bins, and the second one is zoomed in and has more bins to better show the distribution of the simulated data.

```{r}
#| echo: false

y_pdf <- function(y) {1 / (y * sqrt(2 * pi)) * exp(-0.5 * (log(y))^2)}

hist(Y, breaks = 100, freq = F, ylim = c(0, 0.65),
     main = "Simulated data and theoretical PDF of Y", col = "lightseagreen")
curve(y_pdf(x), from = min(Y), to = max(Y), col = "red", lwd = 2, add = TRUE)

hist(Y, breaks = 500, freq = F, ylim = c(0, 0.65), xlim = c(0, 10),
     main = "Simulated data and theoretical PDF of Y, zoomed in", col = "lightseagreen")
curve(y_pdf(x), from = min(Y), to = 10, col = "red", lwd = 2, add = TRUE)
```

### Problem 3c

***Use Monte Carlo simulation with $m=10000$ random draws to estimate $\mathrm{E}(Y)$ where $Y=\exp(X)$ and $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. Check the convergence of the estimate by plotting the sequential Monte Carlo estimates for increasing Monte Carlo sample sizes of $10,20,30,\ldots,9900, 10000$. Does the estimate seem to converge (settle down) to the true expectation, which happens to be $\mathrm{E}(Y)=\exp(\frac{1}{2})$?***

```{r}
#| echo: false

set.seed(123)

Y_mean   <- exp(0.5)
mc_means <- sapply(seq(10, 10000, by = 10), function(n) mean(Y[1:n]))

plot(seq(10, 10000, by = 10), mc_means, type = "l", col = "lightseagreen",
     main = "Monte Carlo Convergence of E(Y)",
     xlab = "Sample Size", ylab = "Estimated Expected Value",
     ylim = range(c(mc_means, Y_mean)))
abline(h = Y_mean, col = "black", lwd = 2, lty = 2)
legend("topright", col = c("lightseagreen", "black"), lwd = 2, lty = c(1, 2),
       legend = c("Monte Carlo Expected Value Estimate", "True Expected Value exp(0.5) ~ 1.6487"))
```

Monte Carlo Simulation relies on repeated random sampling to obtain some estimated value, in this case the expected value of $Y=\exp(X)$ where $X \sim \mathrm{Normal}(\mu = 0, \sigma^2 = 1)$. The plot above shows how the average mean value of $Y$ converges to the true mean (which is approximately 1.6587) as sample size increases. There are 1000 data points in the plot since each data point includes 10 more observations and we have generated random data with 10000 observations in total. We note that after about the 200th data point, which averages 2000 observations, the Monte Carlo estimate of the expected value do not fluctuate as much as it did for smaller sample sizes, and pretty much converges to the true mean.
