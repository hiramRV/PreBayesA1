---
title: "Assignment 1. Statistical Theory and Modelling"
format: html
authors: "Caroline Birkehammar, Pablo Paras Ochoa, Steven Hiram Rubio Vasquez"
editor: visual
---

## Statistical Theory and Modelling - Assignment Part 2

#### Import libraries

```{r,warning=FALSE,message=FALSE}
#| echo: false
library(pacman)
p_load(readxl, tidyverse, ggplot2, janitor, e1071, scales, ggridges, patchwork)

Sys.setlocale("LC_ALL", "es_ES.UTF-8") 
options(scipen = 999) 
```

## Part 4

#### Problem 4a

Letting $a = (1,0,2)^T$ and $b = (1,0,-1)^T$, we perform the dot product.

```{r}
a = c(1,0,2)
b = c(1,0,-1)
(a%*%b)[1]
```

For two orthogonal vectors, the dot product will be equal to 0. Since our result is -1, we conclude that $a$ and $b$ are not orthogonal vectors.

#### Problem 4b

First we simulate a $10x3$ matrix $(X)$ with standard normal $N(0,1)$ random variables and a matrix $b = (1,1,2)^T$.

```{r}
set.seed(22)
X = matrix(rnorm(30, mean = 0, sd = 1), nrow = 10, ncol = 3, byrow = TRUE)
X
```

```{r}
b = c(1, 1, 2)
b
```

Then we perform matrix-vector multiplication, using the operator $%*%$

```{r}
mu = (X%*%b)
mu
```

The first element of the matrix $\mu_{[1,1]}=3.988697$ is obtained by calculating the dot product between the first row of $X$ $(-0.512139088, 2.48518368, 1.0078262)$ and the vector $b$. We can calculate it by hand to corroborate the result.

```{r}
mu_1 = (1*-0.512139088) + (1*2.48518368) + (2*1.0078262)
mu_1
```

#### Problem 4c

We simulate the vector of errors $epsilon$ from a normal distribution with $mean = 0$ and standard $deviation = 0.1$.

```{r}
epsilon = matrix(rnorm(10, mean = 0, sd = 0.1), nrow = 10, ncol = 1, byrow = TRUE)
epsilon
```

With the error vector, we can create a vector of response observations on the variable $Y$:

```{r}
y = mu + epsilon
y
```

Finally, we compute the least squares estimate for our particular vectors

```{r}
b_hat = solve(t(X) %*% X) %*% t(X) %*% y
b_hat

```

This result of our least squares estimation of the coefficients, $\hat{\beta}$, is similar to the input vector that we used, which was $(1, 1, 2)^T$.

#### Problem 4d

For this task, we first calculate the vector or residuals.

```{r}
error = y-X%*%b_hat
error
```

Then we can estimate the variance of the errors as follows.

```{r}
n=10
p=3
var_e = (t(error)%*%error)/(n-p)
var_e[1]
```

At last we can obtain the covariance matrix and extract the square root of the values in the diagonal.

```{r}
CoVar = var_e[1]*solve(t(X) %*% X)
CoVar_ind = matrix(c(sqrt(CoVar[1]),
                     sqrt(CoVar[5]),
                     sqrt(CoVar[9])),
                  nrow=3,ncol=1)
CoVar_ind
```

## Part 5

#### Import data :)

```{r}
data = read.csv("https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv",
header = TRUE)
y = data$nBugs
X = data[,-1]
X = as.matrix(X)
```

#### Problem 5a

If we model our data using a Negative Binomial distribution, $Y$ (nBugs) with a mean estimate of $\hat\mu=\bar y \approx 5.2528$ is the number of Bernoulli trials until $r$ "successes" or "failures". Whether $r$ respresents successes or failures depends on the data and the research question. We do not have access to information about the dataset and what its parameters represent, so we leave out the interpretation of the results in any other aspect than purely statistical.

In this task, we are interested in finding the maximum likelihood estimate of $r$ given our data on $Y$. To do this, we create a function to find the negative log-likelihood function of our data, and then find the minimum value of that function (the optim function in R performs minimization by default, so we negate the log-likelihood function).

```{r}

# Negative binomial log likelihood function
loglik_negbin <- function(r, y){
  return(-sum(dnbinom(y, r, mu = 5.2528, log = TRUE)))
}

opt <- optim(par = 1, gr = NULL, fn = loglik_negbin, y, method = "L-BFGS-B",
             lower = 0.0001, hessian = TRUE)

rhat <- opt$par
```

The maximum likelihood estimate of $r$ given our data is $\hat r \approx 1.4737$.

#### Problem 5b

The task is to find the standard error of the maximum likelihood estimate of $r$, which is the same as the standard deviation in the sampling distribution of the estimator. In large samples (n \> 30) we can approximate the sampling distribution of the estimator with a normal distribution, so that $\hat{r} \sim N\left( r_0, \frac{1}{-l''(\hat r)} \right)$ approximately, and this is what we use to calculate the standard error below.

```{r}

mle_se <- 1 / sqrt(opt$hessian[1])
```

The standard error of our maximum likelihood estimate is approximately $SE_r \approx 0.2875$.

#### Problem 5c

To estimate both $\mu$ and $r$ using the maximum likelihood method we use the same method as before, but insert a vector of the two parameters to be estimated into the log-likelihood function instead of only one parameter. Then we find the best estimators by finding the minimum values of the negative log-likelihood functions.

```{r}

# Joint negative log-likelihood function
loglik_negbin_2 <- function(param) {
  r  <- param[1]
  mu <- param[2]
  return(-sum(dnbinom(y, r, mu = mu, log = TRUE)))
}

opt_2 <- optim(par = c(1, 1), fn = loglik_negbin_2, method = "L-BFGS-B",
               lower = c(0.0001, 0.0001), hessian = TRUE)

rhat  <- opt_2$par[1]
muhat <- opt_2$par[2]
```

The estimated value for $\mu$ is approximately $5.2528$, and the estimated value for $r$ is approximately $1.4737$, the same value that we received before when we estimated only one parameter.

#### Problem 5d

Similarly to before, we use the second derivatives to calculate the standard errors of the parameter estimates. This time they are stored in a Hessian matrix however, so the code has to be adjusted slightly.

```{r}
se_rhat  <- sqrt(solve(opt_2$hessian)[1, 1])
se_muhat <- sqrt(solve(opt_2$hessian)[2, 2])
```

The standard error for the mean estimate $SE_\mu$ is $0.5133$, and the standard error for the $r$ estimate $SE_r$ is $0.2875$. Both standard errors are rounded to four decimals.

## Part 6

### Problem 6a

### Problem 6b

### Problem 6c

## Bonus Problem

## References

Source code: <https://github.com/hiramRV/PreBayesA1>
