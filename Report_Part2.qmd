---
title: "Assignment 2. Statistical Theory and Modelling"
format: html
authors: "Caroline Birkehammar, Pablo Paras Ochoa, Steven Hiram Rubio Vasquez"
editor: visual
---

## Statistical Theory and Modelling - Assignment Part 2

#### Import libraries

```{r,warning=FALSE,message=FALSE}
#| echo: false
library(pacman)
p_load(readxl, tidyverse, ggplot2, janitor, e1071, scales, ggridges, patchwork)

Sys.setlocale("LC_ALL", "es_ES.UTF-8") 
options(scipen = 999) 
```

## Part 4

#### Problem 4a

Letting $a = (1,0,2)^T$ and $b = (1,0,-1)^T$, we perform the dot product.

```{r}
a = c(1,0,2)
b = c(1,0,-1)
(a%*%b)[1]
```

For two orthogonal vectors, the dot product will be equal to 0. Since our result is -1, we conclude that $a$ and $b$ are not orthogonal vectors.

#### Problem 4b

First we simulate a $10x3$ matrix $(X)$ with standard normal $N(0,1)$ random variables and a matrix $b = (1,1,2)^T$.

```{r}
set.seed(22)
X = matrix(rnorm(30, mean = 0, sd = 1), nrow = 10, ncol = 3, byrow = TRUE)
X
```

```{r}
b = c(1, 1, 2)
b
```

Then we perform matrix-vector multiplication, using the operator $%*%$

```{r}
mu = (X%*%b)
mu
```

The first element of the matrix $\mu_{[1,1]}=3.988697$ is obtained by calculating the dot product between the first row of $X$ $(-0.512139088, 2.48518368, 1.0078262)$ and the vector $b$. We can calculate it by hand to corroborate the result.

```{r}
mu_1 = (1*-0.512139088) + (1*2.48518368) + (2*1.0078262)
mu_1
```

#### Problem 4c

We simulate the vector of errors $\epsilon$ from a normal distribution with mean $\mu= 0$ and standard deviation $\sigma= 0.1$.

```{r}
epsilon = matrix(rnorm(10, mean = 0, sd = 0.1), nrow = 10, ncol = 1, byrow = TRUE)
epsilon
```

With the error vector, we can create a vector of response observations on the variable $Y$:

```{r}
y = mu + epsilon
y
```

Finally, we compute the least squares estimate for our particular vectors

```{r}
b_hat = solve(t(X) %*% X) %*% t(X) %*% y
b_hat
```

This result of our least squares estimation of the coefficients, $\hat{\beta}$, is similar to the input vector that we used, which was $(1, 1, 2)^T$.

#### Problem 4d

For this task, we first calculate the vector or residuals.

```{r}
error = y - X%*%b_hat
error
```

Then we estimate the variance of the errors as follows.

```{r}
n = nrow(X)
p = ncol(X)
var_e = (t(error) %*% error)/(n - p)
var_e[1]
```

At last we obtain the covariance matrix and extract the square root of the values in the diagonal.

```{r}
CoVar = var_e[1] * solve(t(X) %*% X)
CoVar_ind = matrix(c(sqrt(CoVar[1]),
                     sqrt(CoVar[5]),
                     sqrt(CoVar[9])),
                  nrow = 1, ncol = 3)
CoVar_ind
```

## Part 5

#### Import data :)

```{r}
data = read.csv("https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv",
header = TRUE)
y = data$nBugs
X = data[,-1]
X = as.matrix(X)
```

#### Problem 5a

If we model our data using a Negative Binomial distribution, $Y$ (nBugs) with a mean estimate of $\hat\mu=\bar y \approx 5.2528$ is the number of Bernoulli trials until $r$ "successes" or "failures". Whether $r$ respresents successes or failures depends on the data and the research question. We do not have access to information about the dataset and what its parameters represent, so we leave out the interpretation of the results in any other aspect than purely statistical.

In this task, we are interested in finding the maximum likelihood estimate of $r$ given our data on $Y$. To do this, we create a function to find the negative log-likelihood function of our data, and then find the minimum value of that function (the optim function in R performs minimization by default, so we negate the log-likelihood function).

```{r}

# Negative binomial log likelihood function
loglik_negbin <- function(r, y){
  return(-sum(dnbinom(y, r, mu = 5.2528, log = TRUE)))
}

opt <- optim(par = 1, gr = NULL, fn = loglik_negbin, y, method = "L-BFGS-B",
             lower = 0.0001, hessian = TRUE)

rhat <- opt$par
rhat
```

The maximum likelihood estimate of $r$ given our data is $\hat r \approx 1.4737$.

#### Problem 5b

The task is to find the standard error of the maximum likelihood estimate of $r$, which is the same as the standard deviation in the sampling distribution of the estimator. In large samples (n \> 30) we can approximate the sampling distribution of the estimator with a normal distribution, so that $\hat{r} \sim N\left( r_0, \frac{1}{-l''(\hat r)} \right)$ approximately, and this is what we use to calculate the standard error below.

```{r}

mle_se <- 1 / sqrt(opt$hessian[1])
mle_se
```

The standard error of our maximum likelihood estimate is approximately $SE_r \approx 0.2875$.

#### Problem 5c

To estimate both $\mu$ and $r$ using the maximum likelihood method we use the same method as before, but insert a vector of the two parameters to be estimated into the log-likelihood function instead of only one parameter. Then we find the best estimators by finding the minimum values of the negative log-likelihood functions.

```{r}

# Joint negative log-likelihood function
loglik_negbin_2 <- function(param) {
  r  <- param[1]
  mu <- param[2]
  return(-sum(dnbinom(y, r, mu = mu, log = TRUE)))
}

opt_2 <- optim(par = c(1, 1), fn = loglik_negbin_2, method = "L-BFGS-B",
               lower = c(0.0001, 0.0001), hessian = TRUE)

rhat  <- opt_2$par[1]
muhat <- opt_2$par[2]

rhat
muhat
```

The estimated value for $\mu$ is approximately $5.2528$, and the estimated value for $r$ is approximately $1.4737$, the same value that we received before when we estimated only one parameter.

#### Problem 5d

Similarly to before, we use the second derivatives to calculate the standard errors of the parameter estimates. This time they are stored in a Hessian matrix however, so the code has to be adjusted slightly.

```{r}
se_rhat  <- sqrt(solve(opt_2$hessian)[1, 1])
se_muhat <- sqrt(solve(opt_2$hessian)[2, 2])

se_rhat
se_muhat
```

The standard error for the mean estimate $SE_\mu$ is $0.5133$, and the standard error for the $r$ estimate $SE_r$ is $0.2875$. Both standard errors are rounded to four decimals.

## Part 6

### Problem 6a

The model fit is in the code below. As can be seen, the estimates obtained through numerical optimization using log likelihood very closely match those obtained using the pre-built glm function.

```{r}

# Modelito is for verification purposes
modelito <- glm(nBugs ~ nCommits + propC + propJava + complexity,
                 family = poisson, data = data)

# This is the actual regression using numerical optimization 
y <- data$nBugs

X <- cbind(data$intercept, data$nCommits, data$propC, data$propJava, data$complexity)

log_like_pois_reg <- function(betas, y, X) {
  lambda <- exp(X %*% betas)
  logLik <- sum(dpois(y, lambda = lambda, log = T))
  return(logLik)
}

init_vals <- c(0, 0, 0, 0, 0)

reg_opt <- optim(init_vals, log_like_pois_reg, gr = NULL, method = c("BFGS"), y, X,
                control = list(fnscale = -1), hessian = T)

b_con_sombrerito <- reg_opt$par


# Comparing both results to ensure the results from our function are accurate 
b_con_sombrerito

summary(modelito)
```

### Problem 6b

Only one of the covariates does not seem to be statistically different from 0 (the number of commits). All other confidence intervals do not cross over 0 but the confidence interval for nCommits does.

```{r}

cov_b_sombrero <- -solve(reg_opt$hessian)

var_b_sombrero <- diag(cov_b_sombrero)

sd_b_sombrero <- sqrt(var_b_sombrero)

menor <- b_con_sombrerito - 1.96*sd_b_sombrero

mayor <- b_con_sombrerito + 1.96*sd_b_sombrero

conf_int <- tibble(
  beta = b_con_sombrerito,
  std_err = sd_b_sombrero,
  lower = menor,
  upper = mayor
)

print(conf_int)

```

### Problem 6c

The predicted number of bugs for release 92 is around bugs 19.15 given both the covariate vector for release 92 and our model.

```{r}

x92 <- c(1, 10, 0.45, 0.5, 0.89)

pred <- exp(x92 %*% b_con_sombrerito)

print(pred)

# Testing with modelito to verify the answer
x92_plus <- data.frame(
  nCommits = 10,
  propC = 0.45,
  propJava = 0.5,
  complexity = 0.89
)

pred <- predict(modelito, newdata = x92_plus, type = "response")

print(pred)
```

## Bonus Problem

## References

Source code: <https://github.com/hiramRV/PreBayesA1>
